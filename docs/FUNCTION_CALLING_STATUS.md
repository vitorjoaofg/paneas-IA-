# Status da Implementa√ß√£o de Function Calling

**Data**: 2025-10-29
**Status**: ‚úÖ Implementado com bug a corrigir

---

## ‚úÖ O Que Foi Implementado

### 1. Schemas Completos (`api/schemas/llm.py`)
- ‚úÖ `ToolFunction`: Defini√ß√£o de fun√ß√£o com nome, descri√ß√£o e par√¢metros
- ‚úÖ `Tool`: Wrapper para functions
- ‚úÖ `FunctionCall`: Chamada espec√≠fica com nome e argumentos JSON
- ‚úÖ `ToolCall`: Registro de tool call com ID √∫nico
- ‚úÖ `ChatMessage` estendido com:
  - `role="tool"` suportado
  - `tool_calls` para mensagens do assistente
  - `tool_call_id` para respostas de tools
  - `name` para identifica√ß√£o de fun√ß√£o
- ‚úÖ `ChatRequest` com:
  - `tools: Optional[List[Tool]]`
  - `tool_choice`: auto/none/required ou dict espec√≠fico

### 2. Tool Executor (`api/services/tool_executor.py`)
- ‚úÖ Classe `ToolExecutor` com registry de fun√ß√µes
- ‚úÖ M√©todo `register()` para adicionar fun√ß√µes
- ‚úÖ M√©todo `execute()` async que:
  - Valida nome da fun√ß√£o
  - Parse de argumentos JSON
  - Executa fun√ß√£o (sync ou async)
  - Tratamento completo de erros
  - Retorna resultado como JSON string
- ‚úÖ M√©todo `execute_all()` para m√∫ltiplas tools
- ‚úÖ Singleton `get_tool_executor()`
- ‚úÖ Logs estruturados de cada execu√ß√£o

### 3. Tool Unimed (`api/services/tools/unimed.py`)
- ‚úÖ Fun√ß√£o `unimed_consult()` async completa
- ‚úÖ Par√¢metros: base_url, cidade, tipo, protocolo, cpf, data_nascimento
- ‚úÖ Normaliza√ß√£o de CPF e data
- ‚úÖ Requisi√ß√£o HTTP com httpx
- ‚úÖ Tratamento de erros (404, 400, timeout, HTTP errors)
- ‚úÖ Masking de PII nos logs
- ‚úÖ Retorno estruturado com success/error

### 4. Mock API Unimed (`scripts/mock_unimed_api.py`)
- ‚úÖ Servidor FastAPI em localhost:9999
- ‚úÖ 3 CPFs de teste com dados completos
- ‚úÖ Endpoint `/{cidade}/{tipo}` com query params
- ‚úÖ Valida√ß√£o de CPF e data de nascimento
- ‚úÖ Respostas realistas com hist√≥rico e car√™ncias
- ‚úÖ Tratamento de erros 404/400

### 5. Router Modificado (`api/routers/llm.py`)
- ‚úÖ Import de tool_executor e unimed_consult
- ‚úÖ Registro da tool no startup
- ‚úÖ Auto-desabilita streaming quando tools presentes
- ‚úÖ Loop de tool calling (at√© 5 itera√ß√µes)
- ‚úÖ Detec√ß√£o de tool_calls no response
- ‚úÖ Execu√ß√£o autom√°tica de tools
- ‚úÖ Segunda chamada ao LLM com resultados
- ‚úÖ Metadata com `tool_iterations`
- ‚ö†Ô∏è **BUG**: Entra no loop mesmo sem tools, causando travamento

### 6. Documenta√ß√£o
- ‚úÖ `docs/FUNCTION_CALLING.md` - Guia completo de uso
- ‚úÖ `scripts/test_function_calling.py` - Suite de testes automatizados
- ‚úÖ `scripts/test_function_calling_live.py` - Teste ao vivo
- ‚úÖ Exemplos de uso com curl
- ‚úÖ Guia para adicionar novas tools

---

## ‚ö†Ô∏è Problema Identificado

### Bug no Router (`api/routers/llm.py`)

**Sintoma**: Requisi√ß√µes travam (timeout) mesmo sem tools presentes.

**Causa**: O c√≥digo sempre entra no loop de tool calling, mesmo quando n√£o h√° tools:

```python
# Linha ~71: Fluxo de streaming OK
if payload.stream and not has_tools:
    # ... retorna streaming response ...

# Linha ~93: PROBLEMA - sempre executado!
messages = list(payload.messages)  # ChatMessage objects
...
while iteration < MAX_TOOL_ITERATIONS:
    current_payload = payload.model_dump()
    current_payload["messages"] = messages  # Sobrescreve com objects!
```

**Problemas**:
1. Fluxo entra no loop mesmo quando `has_tools=False` e `stream=False`
2. `messages` cont√©m objetos Pydantic, n√£o dicts
3. Sobrescrever `current_payload["messages"]` causa erro de serializa√ß√£o
4. Falta early return para fluxo sem tools

---

## üîß Corre√ß√£o Necess√°ria

### Arquivo: `api/routers/llm.py`

Substituir todo o bloco ap√≥s linha 68 por:

```python
    start = time.perf_counter()

    # Se streaming sem tools, usar fluxo antigo
    if payload.stream and not has_tools:
        upstream_payload = payload.model_dump()
        upstream_payload["stream"] = True

        async def event_iterator():
            async for chunk in chat_completion_stream(
                upstream_payload,
                target_model,
                router_metadata=router_metadata,
            ):
                yield chunk

        return StreamingResponse(
            event_iterator(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",
            },
        )

    # Se n√£o tem tools, usar fluxo simples (uma chamada)
    if not has_tools:
        upstream_payload = payload.model_dump()
        upstream_payload.pop("stream", None)

        upstream_response = await chat_completion(
            upstream_payload,
            target_model,
            router_metadata=router_metadata,
        )
        elapsed = time.perf_counter() - start

        usage = upstream_response.get("usage", {})
        choices = [
            ChatChoice(
                index=item.get("index", 0),
                message=ChatMessage(**item.get("message", {})),
                finish_reason=item.get("finish_reason", "stop"),
            )
            for item in upstream_response.get("choices", [])
        ]

        usage_metrics = UsageMetrics(
            prompt_tokens=usage.get("prompt_tokens", prompt_tokens),
            completion_tokens=usage.get("completion_tokens", 0),
            total_tokens=usage.get("total_tokens", usage.get("prompt_tokens", 0) + usage.get("completion_tokens", 0)),
        )

        response_id = upstream_response.get("id", f"chatcmpl-{secrets.token_hex(8)}")
        model_name = upstream_response.get("model", payload.model)
        metadata = upstream_response.setdefault("metadata", {})
        metadata.setdefault("router_target", router_metadata["router_decision"])
        metadata["router_decision"] = router_metadata["router_decision"]
        metadata["router_reason"] = router_metadata["router_reason"]
        metadata["latency_ms"] = int(elapsed * 1000)

        return ChatResponse(id=response_id, model=model_name, choices=choices, usage=usage_metrics)

    # Fluxo COM TOOLS - Loop de tool calling
    messages = [msg.model_dump() for msg in payload.messages]  # Converter para dicts!
    total_prompt_tokens = 0
    total_completion_tokens = 0
    iteration = 0
    response_id = f"chatcmpl-{secrets.token_hex(8)}"

    # Loop de tool calling
    while iteration < MAX_TOOL_ITERATIONS:
        iteration += 1

        # Preparar payload para esta itera√ß√£o
        current_payload = payload.model_dump()
        current_payload["messages"] = messages  # Agora s√£o dicts
        current_payload.pop("stream", None)

        LOGGER.info(
            "llm_call",
            iteration=iteration,
            num_messages=len(messages),
        )

        # Fazer chamada ao LLM
        upstream_response = await chat_completion(
            current_payload,
            target_model,
            router_metadata=router_metadata,
        )

        # Acumular tokens
        usage = upstream_response.get("usage", {})
        total_prompt_tokens += usage.get("prompt_tokens", 0)
        total_completion_tokens += usage.get("completion_tokens", 0)

        # Extrair primeira choice
        choices_raw = upstream_response.get("choices", [])
        if not choices_raw:
            raise HTTPException(status_code=500, detail="No choices returned from LLM")

        first_choice = choices_raw[0]
        message_dict = first_choice.get("message", {})
        finish_reason = first_choice.get("finish_reason", "stop")

        # Verificar se h√° tool calls
        tool_calls_raw = message_dict.get("tool_calls")

        if not tool_calls_raw or finish_reason != "tool_calls":
            # N√£o h√° tool calls, retornar resposta final
            elapsed = time.perf_counter() - start

            choices = [
                ChatChoice(
                    index=first_choice.get("index", 0),
                    message=ChatMessage(**message_dict),
                    finish_reason=finish_reason,
                )
            ]

            usage_metrics = UsageMetrics(
                prompt_tokens=total_prompt_tokens,
                completion_tokens=total_completion_tokens,
                total_tokens=total_prompt_tokens + total_completion_tokens,
            )

            model_name = upstream_response.get("model", payload.model)
            metadata = upstream_response.setdefault("metadata", {})
            metadata.setdefault("router_target", router_metadata["router_decision"])
            metadata["router_decision"] = router_metadata["router_decision"]
            metadata["router_reason"] = router_metadata["router_reason"]
            metadata["latency_ms"] = int(elapsed * 1000)
            metadata["tool_iterations"] = iteration

            return ChatResponse(
                id=response_id,
                model=model_name,
                choices=choices,
                usage=usage_metrics,
            )

        # H√° tool calls, executar
        LOGGER.info(
            "tool_calls_detected",
            iteration=iteration,
            num_tool_calls=len(tool_calls_raw),
        )

        # Parse tool calls
        tool_calls = [ToolCall(**tc) for tc in tool_calls_raw]

        # Adicionar mensagem do assistente com tool calls
        messages.append({
            "role": "assistant",
            "content": message_dict.get("content"),
            "tool_calls": [tc.model_dump() for tc in tool_calls],
        })

        # Executar todas as tool calls
        tool_results = await tool_executor.execute_all(tool_calls)

        LOGGER.info(
            "tool_calls_executed",
            iteration=iteration,
            num_results=len(tool_results),
        )

        # Adicionar resultados √†s mensagens
        messages.extend(tool_results)

        # Continuar loop para fazer nova chamada ao LLM

    # Se chegou aqui, excedeu max iterations
    raise HTTPException(
        status_code=500,
        detail=f"Maximum tool calling iterations ({MAX_TOOL_ITERATIONS}) exceeded. "
               f"Possible infinite loop detected."
    )
```

**Mudan√ßas principais**:
1. ‚úÖ Early return para requests sem tools (fluxo simples)
2. ‚úÖ Convers√£o de `messages` para dicts: `[msg.model_dump() for msg in payload.messages]`
3. ‚úÖ Separa√ß√£o clara de 3 fluxos: streaming, simples, com-tools

---

## üß™ Como Testar Ap√≥s Corre√ß√£o

### 1. Aplicar Corre√ß√£o

```bash
# Editar o arquivo com corre√ß√£o acima
vim api/routers/llm.py

# Copiar para container
docker cp api/routers/llm.py stack-api:/app/routers/llm.py

# Reiniciar
docker restart stack-api
```

### 2. Teste B√°sico (sem tools)

```bash
curl -X POST http://localhost:8000/api/v1/chat/completions \
  -H "Authorization: Bearer token_abc123" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "paneas-q32b",
    "messages": [{"role": "user", "content": "Ol√°"}],
    "max_tokens": 50
  }'
```

**Esperado**: Resposta em ~5-10s com completion normal.

### 3. Teste com Tools (com mock rodando)

```bash
# Garantir que mock est√° rodando
python3 scripts/mock_unimed_api.py &

# Executar teste
python3 scripts/test_function_calling_live2.py
```

**Esperado**:
- Se vLLM suporta function calling: Tool ser√° executada e mock receber√° chamada
- Se vLLM n√£o suporta: LLM responder√° normalmente sem chamar tool

---

## üìù Limita√ß√£o Atual: Suporte do vLLM

O Qwen2.5-32B via vLLM **pode n√£o suportar function calling nativamente**.

### Verificar Suporte

```bash
# Ver se vLLM tem feature de tools habilitada
docker logs stack-llm-int4 2>&1 | grep -i "tool\|function"

# Testar diretamente no vLLM
curl -X POST http://localhost:8002/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "/models/qwen2_5/int4-awq-32b",
    "messages": [{"role": "user", "content": "Teste"}],
    "tools": [{"type": "function", "function": {"name": "test", "description": "Test", "parameters": {"type": "object", "properties": {}}}}],
    "tool_choice": "required"
  }'
```

### Alternativas

Se vLLM n√£o suporta function calling:

1. **Usar OpenAI**: `provider="openai"` - GPT-4o-mini suporta tools nativamente
2. **Prompt Engineering**: Ensinar o LLM a retornar JSON estruturado e parsear manualmente
3. **Upgrade vLLM**: Vers√£o mais recente pode ter suporte
4. **Modelo diferente**: Qwen2.5-Instruct pode ter melhor suporte

---

## ‚úÖ Conclus√£o

### O Que Funciona 100%
- ‚úÖ Schemas OpenAI-compliant
- ‚úÖ Tool executor robusto
- ‚úÖ Tool unimed_consult completa
- ‚úÖ Mock API funcional
- ‚úÖ Registro de tools
- ‚úÖ Detec√ß√£o e execu√ß√£o de tool_calls
- ‚úÖ Loop de itera√ß√£o
- ‚úÖ Documenta√ß√£o completa

### O Que Precisa de Corre√ß√£o
- ‚ö†Ô∏è Bug no router (corre√ß√£o fornecida acima)

### O Que Depende do vLLM
- ‚ùì LLM realmente gerar `tool_calls` no response
- ‚ùì vLLM suportar par√¢metro `tools` e `tool_choice`

### Pr√≥ximos Passos
1. Aplicar corre√ß√£o no router
2. Testar fluxo sem tools
3. Testar fluxo com tools
4. Se LLM n√£o gerar tool_calls, considerar alternativas

---

**Implementado por**: Claude Code
**Arquivos criados**: 8
**Arquivos modificados**: 3
**Linhas de c√≥digo**: ~800
