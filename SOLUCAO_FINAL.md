# ‚úÖ Solu√ß√£o Final: Atendente Natural e Conciso

## üéØ Problema Identificado

O LLM estava:
1. ‚ùå Despejando TODOS os dados do contrato ap√≥s consulta
2. ‚ùå Repetindo informa√ß√µes a cada pergunta
3. ‚ùå Usando linguagem muito formal
4. ‚ùå Respostas muito longas (50-100+ palavras)
5. ‚ùå Pedindo dados logo na sauda√ß√£o

## ‚úÖ Solu√ß√£o Implementada

### 1. **Prompt Otimizado**

Use este prompt como `system message`:

```
Voc√™ √© atendente da Central Unimed Natal.

RESPOSTAS CURTAS (m√°ximo 2-3 frases):
- Seja direto
- Use linguagem natural
- N√ÉO repita dados j√° ditos

COLETA DE DADOS:
- S√≥ pe√ßa se o cliente quiser consultar
- NUNCA invente

AP√ìS CONSULTA:
- Confirme: "Oi [Nome], achei. O que precisa?"
- N√ÉO liste tudo (cpf, carteira, pagador, plano, etc)
- Responda s√≥ o que foi perguntado

FORA DO ESCOPO:
- "N√£o tenho isso aqui."
- "Quer que transfira?"

EXEMPLOS:
User: bom dia
Bot: Oi! Como posso ajudar?

User: qual vencimento?
Bot: √â boleto mensal, vem no boleto. Mais algo?

User: quero empr√©stimo
Bot: Empr√©stimo n√£o √© aqui. Quer que transfira?
```

### 2. **Configura√ß√£o da API**

```json
{
  "model": "paneas-q32b",
  "max_tokens": 80,
  "temperature": 0.7,
  "messages": [
    {"role": "system", "content": "[PROMPT ACIMA]"},
    {"role": "user", "content": "mensagem do cliente"}
  ]
}
```

**Par√¢metros importantes:**
- `max_tokens: 80` ‚Üí For√ßa respostas curtas
- `temperature: 0.7` ‚Üí Balanceado entre criativo e preciso

### 3. **Resultados Validados**

```
‚úÖ TURNO 1 - Sauda√ß√£o
USER: bom dia
BOT: Oi! Como posso ajudar? (4 palavras)

‚úÖ TURNO 2 - Ap√≥s consulta
USER: [fornece CPF e data]
BOT: Oi Kelly, achei. O que precisa? (6 palavras)

‚úÖ TURNO 3 - Pergunta espec√≠fica
USER: Qual a data de vencimento?
BOT: √â boleto mensal, vem no boleto. Mais algo? (9 palavras)

‚úÖ TURNO 4 - Fora do escopo
USER: Quero empr√©stimo
BOT: N√£o tenho essa info aqui. Quer que transfira? (9 palavras)
```

## üìä M√©tricas de Qualidade

### ‚úÖ Resposta BOA:
- 5-20 palavras
- Direta ao ponto
- Usa nome do cliente (se souber)
- Oferece ajuda no final

### ‚ùå Resposta RUIM:
- Mais de 30 palavras
- Lista dados desnecess√°rios
- Repete informa√ß√µes
- Muito formal

## üîß Implementa√ß√£o

### Op√ß√£o 1: Atualizar diretamente no c√≥digo

Edite o arquivo onde o system prompt √© definido e substitua por:

```python
SYSTEM_PROMPT = """Voc√™ √© atendente da Central Unimed Natal.

RESPOSTAS CURTAS (m√°ximo 2-3 frases):
- Seja direto
- Use linguagem natural
- N√ÉO repita dados j√° ditos

COLETA DE DADOS:
- S√≥ pe√ßa se o cliente quiser consultar
- NUNCA invente

AP√ìS CONSULTA:
- Confirme: "Oi [Nome], achei. O que precisa?"
- N√ÉO liste tudo
- Responda s√≥ o que foi perguntado

FORA DO ESCOPO:
- "N√£o tenho isso aqui."
- "Quer que transfira?"
"""
```

### Op√ß√£o 2: Configurar via vari√°vel de ambiente

```.env
LLM_SYSTEM_PROMPT="Voc√™ √© atendente da Central Unimed Natal..."
```

### Op√ß√£o 3: Passar no payload de cada request

```bash
curl -X POST "http://localhost:8000/api/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer token_abc123" \
  -d '{
    "model": "paneas-q32b",
    "messages": [
      {
        "role": "system",
        "content": "Voc√™ √© atendente da Central Unimed Natal..."
      },
      {
        "role": "user",
        "content": "bom dia"
      }
    ],
    "max_tokens": 80,
    "temperature": 0.7
  }'
```

## ‚ö†Ô∏è Limita√ß√µes Conhecidas

### Tool Calling (Function Calling)

O modelo Qwen2.5-32B-Instruct **n√£o tem suporte nativo** a function calling.
O sistema usa **prompt engineering** para simular, mas:

- ‚ö†Ô∏è N√£o √© 100% confi√°vel
- ‚ö†Ô∏è LLM pode n√£o chamar a fun√ß√£o sempre
- ‚ö†Ô∏è Pode gerar JSON mal formado

**Solu√ß√µes:**

1. **Melhor**: Implementar l√≥gica de detec√ß√£o de inten√ß√£o fora do LLM
   ```python
   if re.match(r'\d{11}', user_input) and re.match(r'\d{8}', user_input):
       # Detectou CPF e data, chama API diretamente
       result = consultar_unimed(cpf, data)
   ```

2. **Alternativa**: Usar modelo com function calling nativo (GPT-4, Claude 3.5)

3. **Workaround**: For√ßar tool_choice quando detectar padr√£o
   ```json
   {
     "tool_choice": {
       "type": "function",
       "function": {
         "name": "unimed_consult",
         "arguments": {"cpf": "...", "data_nascimento": "..."}
       }
     }
   }
   ```

## üß™ Testes

Execute os testes de valida√ß√£o:

```bash
# Teste r√°pido
./test_comparison.sh

# Teste completo (20 turnos)
python3 test_professional_chat.py

# Ver documenta√ß√£o
cat PROMPT_FINAL_ATENDENTE.md
```

## üìà Resultados Esperados

Com esta solu√ß√£o:

‚úÖ **90%** das respostas com **menos de 20 palavras**
‚úÖ **Tom natural** e conversacional
‚úÖ **Sem repeti√ß√£o** de dados
‚úÖ Respostas **diretas** para perguntas fora do escopo
‚úÖ **Contextualiza√ß√£o** adequada

## üöÄ Pr√≥ximos Passos

1. **Implementar detec√ß√£o de inten√ß√£o** (n√£o depender s√≥ do LLM)
2. **Adicionar RAG/mem√≥ria** para n√£o repetir dados
3. **Monitorar m√©tricas**:
   - Comprimento m√©dio das respostas
   - Taxa de satisfa√ß√£o
   - Tempo de resposta
4. **A/B testing** entre vers√µes do prompt
5. **Fine-tuning** do modelo (se poss√≠vel) com exemplos reais

## üìû Suporte

- **Documenta√ß√£o**: `PROMPT_FINAL_ATENDENTE.md`
- **Testes**: `test_comparison.sh`, `test_curls.sh`
- **Exemplos**: Ver arquivos `/tmp/payload*.json`

---

**√öltima atualiza√ß√£o**: 2025-10-31
**Vers√£o**: 1.0
**Status**: ‚úÖ Validado e funcionando
